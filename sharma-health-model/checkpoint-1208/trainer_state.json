{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1208,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008278145695364239,
      "grad_norm": 8.204465866088867,
      "learning_rate": 1.9834437086092716e-05,
      "loss": 4.1102,
      "step": 10
    },
    {
      "epoch": 0.016556291390728478,
      "grad_norm": 4.4165568351745605,
      "learning_rate": 1.9668874172185433e-05,
      "loss": 2.4457,
      "step": 20
    },
    {
      "epoch": 0.024834437086092714,
      "grad_norm": 4.375901699066162,
      "learning_rate": 1.9503311258278147e-05,
      "loss": 2.3163,
      "step": 30
    },
    {
      "epoch": 0.033112582781456956,
      "grad_norm": 3.871309518814087,
      "learning_rate": 1.9337748344370864e-05,
      "loss": 2.0792,
      "step": 40
    },
    {
      "epoch": 0.041390728476821195,
      "grad_norm": 3.6167969703674316,
      "learning_rate": 1.917218543046358e-05,
      "loss": 1.9036,
      "step": 50
    },
    {
      "epoch": 0.04966887417218543,
      "grad_norm": 3.9699792861938477,
      "learning_rate": 1.9006622516556292e-05,
      "loss": 1.7119,
      "step": 60
    },
    {
      "epoch": 0.057947019867549666,
      "grad_norm": 5.224239349365234,
      "learning_rate": 1.8841059602649006e-05,
      "loss": 1.6984,
      "step": 70
    },
    {
      "epoch": 0.06622516556291391,
      "grad_norm": 3.455251693725586,
      "learning_rate": 1.8675496688741724e-05,
      "loss": 1.6273,
      "step": 80
    },
    {
      "epoch": 0.07450331125827815,
      "grad_norm": 3.8575847148895264,
      "learning_rate": 1.8509933774834438e-05,
      "loss": 1.5189,
      "step": 90
    },
    {
      "epoch": 0.08278145695364239,
      "grad_norm": 3.7455708980560303,
      "learning_rate": 1.8344370860927155e-05,
      "loss": 1.5401,
      "step": 100
    },
    {
      "epoch": 0.09105960264900662,
      "grad_norm": 3.524066925048828,
      "learning_rate": 1.817880794701987e-05,
      "loss": 1.477,
      "step": 110
    },
    {
      "epoch": 0.09933774834437085,
      "grad_norm": 3.61419415473938,
      "learning_rate": 1.8013245033112587e-05,
      "loss": 1.4041,
      "step": 120
    },
    {
      "epoch": 0.1076158940397351,
      "grad_norm": 4.116690158843994,
      "learning_rate": 1.7847682119205297e-05,
      "loss": 1.5964,
      "step": 130
    },
    {
      "epoch": 0.11589403973509933,
      "grad_norm": 3.2473487854003906,
      "learning_rate": 1.7682119205298015e-05,
      "loss": 1.4565,
      "step": 140
    },
    {
      "epoch": 0.12417218543046357,
      "grad_norm": 5.237165927886963,
      "learning_rate": 1.751655629139073e-05,
      "loss": 1.4192,
      "step": 150
    },
    {
      "epoch": 0.13245033112582782,
      "grad_norm": 3.4509079456329346,
      "learning_rate": 1.7350993377483446e-05,
      "loss": 1.5339,
      "step": 160
    },
    {
      "epoch": 0.14072847682119205,
      "grad_norm": 3.681600332260132,
      "learning_rate": 1.718543046357616e-05,
      "loss": 1.3893,
      "step": 170
    },
    {
      "epoch": 0.1490066225165563,
      "grad_norm": 3.803668260574341,
      "learning_rate": 1.7019867549668878e-05,
      "loss": 1.4759,
      "step": 180
    },
    {
      "epoch": 0.15728476821192053,
      "grad_norm": 3.8749959468841553,
      "learning_rate": 1.685430463576159e-05,
      "loss": 1.28,
      "step": 190
    },
    {
      "epoch": 0.16556291390728478,
      "grad_norm": 3.7438037395477295,
      "learning_rate": 1.6688741721854306e-05,
      "loss": 1.3722,
      "step": 200
    },
    {
      "epoch": 0.173841059602649,
      "grad_norm": 3.9951884746551514,
      "learning_rate": 1.652317880794702e-05,
      "loss": 1.4278,
      "step": 210
    },
    {
      "epoch": 0.18211920529801323,
      "grad_norm": 4.709882736206055,
      "learning_rate": 1.6357615894039737e-05,
      "loss": 1.4665,
      "step": 220
    },
    {
      "epoch": 0.19039735099337748,
      "grad_norm": 3.8799312114715576,
      "learning_rate": 1.619205298013245e-05,
      "loss": 1.4419,
      "step": 230
    },
    {
      "epoch": 0.1986754966887417,
      "grad_norm": 3.3007383346557617,
      "learning_rate": 1.6026490066225165e-05,
      "loss": 1.2992,
      "step": 240
    },
    {
      "epoch": 0.20695364238410596,
      "grad_norm": 3.7275946140289307,
      "learning_rate": 1.5860927152317882e-05,
      "loss": 1.3158,
      "step": 250
    },
    {
      "epoch": 0.2152317880794702,
      "grad_norm": 3.1053664684295654,
      "learning_rate": 1.5695364238410596e-05,
      "loss": 1.2523,
      "step": 260
    },
    {
      "epoch": 0.22350993377483444,
      "grad_norm": 3.3889830112457275,
      "learning_rate": 1.5529801324503314e-05,
      "loss": 1.3263,
      "step": 270
    },
    {
      "epoch": 0.23178807947019867,
      "grad_norm": 3.5042712688446045,
      "learning_rate": 1.5364238410596028e-05,
      "loss": 1.4643,
      "step": 280
    },
    {
      "epoch": 0.24006622516556292,
      "grad_norm": 3.7020821571350098,
      "learning_rate": 1.5198675496688744e-05,
      "loss": 1.2481,
      "step": 290
    },
    {
      "epoch": 0.24834437086092714,
      "grad_norm": 4.021546840667725,
      "learning_rate": 1.5033112582781458e-05,
      "loss": 1.3275,
      "step": 300
    },
    {
      "epoch": 0.25662251655629137,
      "grad_norm": 3.543347120285034,
      "learning_rate": 1.4867549668874173e-05,
      "loss": 1.4413,
      "step": 310
    },
    {
      "epoch": 0.26490066225165565,
      "grad_norm": 5.003645420074463,
      "learning_rate": 1.4701986754966889e-05,
      "loss": 1.5266,
      "step": 320
    },
    {
      "epoch": 0.2731788079470199,
      "grad_norm": 3.1649627685546875,
      "learning_rate": 1.4536423841059603e-05,
      "loss": 1.2558,
      "step": 330
    },
    {
      "epoch": 0.2814569536423841,
      "grad_norm": 3.4198455810546875,
      "learning_rate": 1.4370860927152319e-05,
      "loss": 1.2904,
      "step": 340
    },
    {
      "epoch": 0.2897350993377483,
      "grad_norm": 3.5342724323272705,
      "learning_rate": 1.4205298013245034e-05,
      "loss": 1.5309,
      "step": 350
    },
    {
      "epoch": 0.2980132450331126,
      "grad_norm": 3.384044885635376,
      "learning_rate": 1.403973509933775e-05,
      "loss": 1.196,
      "step": 360
    },
    {
      "epoch": 0.30629139072847683,
      "grad_norm": 4.798773288726807,
      "learning_rate": 1.3874172185430466e-05,
      "loss": 1.4926,
      "step": 370
    },
    {
      "epoch": 0.31456953642384106,
      "grad_norm": 3.525055408477783,
      "learning_rate": 1.3708609271523178e-05,
      "loss": 1.2062,
      "step": 380
    },
    {
      "epoch": 0.3228476821192053,
      "grad_norm": 3.8358497619628906,
      "learning_rate": 1.3543046357615894e-05,
      "loss": 1.1477,
      "step": 390
    },
    {
      "epoch": 0.33112582781456956,
      "grad_norm": 3.4001312255859375,
      "learning_rate": 1.337748344370861e-05,
      "loss": 1.2279,
      "step": 400
    },
    {
      "epoch": 0.3394039735099338,
      "grad_norm": 4.025893688201904,
      "learning_rate": 1.3211920529801325e-05,
      "loss": 1.3581,
      "step": 410
    },
    {
      "epoch": 0.347682119205298,
      "grad_norm": 3.249817371368408,
      "learning_rate": 1.3046357615894041e-05,
      "loss": 1.3249,
      "step": 420
    },
    {
      "epoch": 0.35596026490066224,
      "grad_norm": 4.264159679412842,
      "learning_rate": 1.2880794701986757e-05,
      "loss": 1.2627,
      "step": 430
    },
    {
      "epoch": 0.36423841059602646,
      "grad_norm": 3.5489120483398438,
      "learning_rate": 1.2715231788079472e-05,
      "loss": 1.2791,
      "step": 440
    },
    {
      "epoch": 0.37251655629139074,
      "grad_norm": 4.008798599243164,
      "learning_rate": 1.2549668874172188e-05,
      "loss": 1.179,
      "step": 450
    },
    {
      "epoch": 0.38079470198675497,
      "grad_norm": 4.119330883026123,
      "learning_rate": 1.2384105960264902e-05,
      "loss": 1.5845,
      "step": 460
    },
    {
      "epoch": 0.3890728476821192,
      "grad_norm": 3.664156913757324,
      "learning_rate": 1.2218543046357616e-05,
      "loss": 1.3502,
      "step": 470
    },
    {
      "epoch": 0.3973509933774834,
      "grad_norm": 2.961650848388672,
      "learning_rate": 1.2052980132450332e-05,
      "loss": 1.1579,
      "step": 480
    },
    {
      "epoch": 0.4056291390728477,
      "grad_norm": 3.9449868202209473,
      "learning_rate": 1.1887417218543048e-05,
      "loss": 1.2701,
      "step": 490
    },
    {
      "epoch": 0.4139072847682119,
      "grad_norm": 3.8144609928131104,
      "learning_rate": 1.1721854304635763e-05,
      "loss": 1.2513,
      "step": 500
    },
    {
      "epoch": 0.42218543046357615,
      "grad_norm": 3.24739670753479,
      "learning_rate": 1.1556291390728477e-05,
      "loss": 1.3426,
      "step": 510
    },
    {
      "epoch": 0.4304635761589404,
      "grad_norm": 3.213963508605957,
      "learning_rate": 1.1390728476821193e-05,
      "loss": 1.1699,
      "step": 520
    },
    {
      "epoch": 0.43874172185430466,
      "grad_norm": 2.863612651824951,
      "learning_rate": 1.1225165562913909e-05,
      "loss": 1.3789,
      "step": 530
    },
    {
      "epoch": 0.4470198675496689,
      "grad_norm": 4.417568206787109,
      "learning_rate": 1.1059602649006624e-05,
      "loss": 1.3808,
      "step": 540
    },
    {
      "epoch": 0.4552980132450331,
      "grad_norm": 4.245528221130371,
      "learning_rate": 1.0894039735099338e-05,
      "loss": 1.3212,
      "step": 550
    },
    {
      "epoch": 0.46357615894039733,
      "grad_norm": 2.665832281112671,
      "learning_rate": 1.0728476821192052e-05,
      "loss": 1.3544,
      "step": 560
    },
    {
      "epoch": 0.4718543046357616,
      "grad_norm": 4.180148601531982,
      "learning_rate": 1.0562913907284768e-05,
      "loss": 1.259,
      "step": 570
    },
    {
      "epoch": 0.48013245033112584,
      "grad_norm": 3.751373767852783,
      "learning_rate": 1.0397350993377484e-05,
      "loss": 1.3016,
      "step": 580
    },
    {
      "epoch": 0.48841059602649006,
      "grad_norm": 3.809002161026001,
      "learning_rate": 1.02317880794702e-05,
      "loss": 1.2492,
      "step": 590
    },
    {
      "epoch": 0.4966887417218543,
      "grad_norm": 4.198840141296387,
      "learning_rate": 1.0066225165562915e-05,
      "loss": 1.3778,
      "step": 600
    },
    {
      "epoch": 0.5049668874172185,
      "grad_norm": 3.752668619155884,
      "learning_rate": 9.90066225165563e-06,
      "loss": 1.2119,
      "step": 610
    },
    {
      "epoch": 0.5132450331125827,
      "grad_norm": 3.3350703716278076,
      "learning_rate": 9.735099337748345e-06,
      "loss": 1.2826,
      "step": 620
    },
    {
      "epoch": 0.5215231788079471,
      "grad_norm": 4.272318363189697,
      "learning_rate": 9.56953642384106e-06,
      "loss": 1.3286,
      "step": 630
    },
    {
      "epoch": 0.5298013245033113,
      "grad_norm": 3.8839194774627686,
      "learning_rate": 9.403973509933776e-06,
      "loss": 1.1345,
      "step": 640
    },
    {
      "epoch": 0.5380794701986755,
      "grad_norm": 5.136929512023926,
      "learning_rate": 9.238410596026492e-06,
      "loss": 1.2859,
      "step": 650
    },
    {
      "epoch": 0.5463576158940397,
      "grad_norm": 3.902716636657715,
      "learning_rate": 9.072847682119206e-06,
      "loss": 1.2943,
      "step": 660
    },
    {
      "epoch": 0.554635761589404,
      "grad_norm": 3.864722490310669,
      "learning_rate": 8.907284768211922e-06,
      "loss": 1.3482,
      "step": 670
    },
    {
      "epoch": 0.5629139072847682,
      "grad_norm": 4.1689300537109375,
      "learning_rate": 8.741721854304637e-06,
      "loss": 1.2963,
      "step": 680
    },
    {
      "epoch": 0.5711920529801324,
      "grad_norm": 4.556112289428711,
      "learning_rate": 8.576158940397351e-06,
      "loss": 1.183,
      "step": 690
    },
    {
      "epoch": 0.5794701986754967,
      "grad_norm": 3.6657538414001465,
      "learning_rate": 8.410596026490067e-06,
      "loss": 1.2152,
      "step": 700
    },
    {
      "epoch": 0.5877483443708609,
      "grad_norm": 3.407785654067993,
      "learning_rate": 8.245033112582781e-06,
      "loss": 1.1276,
      "step": 710
    },
    {
      "epoch": 0.5960264900662252,
      "grad_norm": 2.9908018112182617,
      "learning_rate": 8.079470198675497e-06,
      "loss": 1.0649,
      "step": 720
    },
    {
      "epoch": 0.6043046357615894,
      "grad_norm": 4.579558849334717,
      "learning_rate": 7.913907284768213e-06,
      "loss": 1.3963,
      "step": 730
    },
    {
      "epoch": 0.6125827814569537,
      "grad_norm": 3.8739864826202393,
      "learning_rate": 7.748344370860927e-06,
      "loss": 1.3076,
      "step": 740
    },
    {
      "epoch": 0.6208609271523179,
      "grad_norm": 4.058252334594727,
      "learning_rate": 7.582781456953643e-06,
      "loss": 1.2472,
      "step": 750
    },
    {
      "epoch": 0.6291390728476821,
      "grad_norm": 4.079812526702881,
      "learning_rate": 7.417218543046358e-06,
      "loss": 1.2938,
      "step": 760
    },
    {
      "epoch": 0.6374172185430463,
      "grad_norm": 4.192352294921875,
      "learning_rate": 7.251655629139074e-06,
      "loss": 1.3098,
      "step": 770
    },
    {
      "epoch": 0.6456953642384106,
      "grad_norm": 3.3359766006469727,
      "learning_rate": 7.086092715231789e-06,
      "loss": 1.0342,
      "step": 780
    },
    {
      "epoch": 0.6539735099337748,
      "grad_norm": 3.529634475708008,
      "learning_rate": 6.9205298013245035e-06,
      "loss": 1.3427,
      "step": 790
    },
    {
      "epoch": 0.6622516556291391,
      "grad_norm": 3.0519018173217773,
      "learning_rate": 6.754966887417219e-06,
      "loss": 1.1775,
      "step": 800
    },
    {
      "epoch": 0.6705298013245033,
      "grad_norm": 3.758233070373535,
      "learning_rate": 6.589403973509935e-06,
      "loss": 1.2614,
      "step": 810
    },
    {
      "epoch": 0.6788079470198676,
      "grad_norm": 4.14661169052124,
      "learning_rate": 6.423841059602649e-06,
      "loss": 1.4509,
      "step": 820
    },
    {
      "epoch": 0.6870860927152318,
      "grad_norm": 2.859272003173828,
      "learning_rate": 6.258278145695365e-06,
      "loss": 1.1725,
      "step": 830
    },
    {
      "epoch": 0.695364238410596,
      "grad_norm": 3.8133628368377686,
      "learning_rate": 6.09271523178808e-06,
      "loss": 1.1868,
      "step": 840
    },
    {
      "epoch": 0.7036423841059603,
      "grad_norm": 3.174492835998535,
      "learning_rate": 5.927152317880795e-06,
      "loss": 1.2801,
      "step": 850
    },
    {
      "epoch": 0.7119205298013245,
      "grad_norm": 4.026177406311035,
      "learning_rate": 5.76158940397351e-06,
      "loss": 1.1578,
      "step": 860
    },
    {
      "epoch": 0.7201986754966887,
      "grad_norm": 3.4270026683807373,
      "learning_rate": 5.596026490066226e-06,
      "loss": 1.1813,
      "step": 870
    },
    {
      "epoch": 0.7284768211920529,
      "grad_norm": 4.6102752685546875,
      "learning_rate": 5.430463576158941e-06,
      "loss": 1.1888,
      "step": 880
    },
    {
      "epoch": 0.7367549668874173,
      "grad_norm": 3.9669270515441895,
      "learning_rate": 5.264900662251656e-06,
      "loss": 1.3095,
      "step": 890
    },
    {
      "epoch": 0.7450331125827815,
      "grad_norm": 3.9618513584136963,
      "learning_rate": 5.099337748344372e-06,
      "loss": 1.0929,
      "step": 900
    },
    {
      "epoch": 0.7533112582781457,
      "grad_norm": 3.212369203567505,
      "learning_rate": 4.933774834437087e-06,
      "loss": 1.1648,
      "step": 910
    },
    {
      "epoch": 0.7615894039735099,
      "grad_norm": 2.6883111000061035,
      "learning_rate": 4.768211920529802e-06,
      "loss": 1.1474,
      "step": 920
    },
    {
      "epoch": 0.7698675496688742,
      "grad_norm": 3.4286770820617676,
      "learning_rate": 4.6026490066225174e-06,
      "loss": 1.2302,
      "step": 930
    },
    {
      "epoch": 0.7781456953642384,
      "grad_norm": 4.025286674499512,
      "learning_rate": 4.437086092715232e-06,
      "loss": 1.1301,
      "step": 940
    },
    {
      "epoch": 0.7864238410596026,
      "grad_norm": 3.813267946243286,
      "learning_rate": 4.271523178807947e-06,
      "loss": 1.3798,
      "step": 950
    },
    {
      "epoch": 0.7947019867549668,
      "grad_norm": 3.5645837783813477,
      "learning_rate": 4.105960264900663e-06,
      "loss": 1.2216,
      "step": 960
    },
    {
      "epoch": 0.8029801324503312,
      "grad_norm": 3.2669317722320557,
      "learning_rate": 3.940397350993378e-06,
      "loss": 1.2153,
      "step": 970
    },
    {
      "epoch": 0.8112582781456954,
      "grad_norm": 3.5229997634887695,
      "learning_rate": 3.774834437086093e-06,
      "loss": 1.2926,
      "step": 980
    },
    {
      "epoch": 0.8195364238410596,
      "grad_norm": 4.8312811851501465,
      "learning_rate": 3.6092715231788083e-06,
      "loss": 1.3754,
      "step": 990
    },
    {
      "epoch": 0.8278145695364238,
      "grad_norm": 3.6396148204803467,
      "learning_rate": 3.443708609271523e-06,
      "loss": 1.3278,
      "step": 1000
    },
    {
      "epoch": 0.8360927152317881,
      "grad_norm": 3.5276126861572266,
      "learning_rate": 3.278145695364239e-06,
      "loss": 1.2944,
      "step": 1010
    },
    {
      "epoch": 0.8443708609271523,
      "grad_norm": 3.538656711578369,
      "learning_rate": 3.1125827814569537e-06,
      "loss": 1.179,
      "step": 1020
    },
    {
      "epoch": 0.8526490066225165,
      "grad_norm": 3.2313930988311768,
      "learning_rate": 2.947019867549669e-06,
      "loss": 1.078,
      "step": 1030
    },
    {
      "epoch": 0.8609271523178808,
      "grad_norm": 3.8606631755828857,
      "learning_rate": 2.7814569536423843e-06,
      "loss": 1.1605,
      "step": 1040
    },
    {
      "epoch": 0.8692052980132451,
      "grad_norm": 3.0427753925323486,
      "learning_rate": 2.6158940397350995e-06,
      "loss": 1.189,
      "step": 1050
    },
    {
      "epoch": 0.8774834437086093,
      "grad_norm": 3.5097908973693848,
      "learning_rate": 2.450331125827815e-06,
      "loss": 1.2088,
      "step": 1060
    },
    {
      "epoch": 0.8857615894039735,
      "grad_norm": 3.657503366470337,
      "learning_rate": 2.28476821192053e-06,
      "loss": 1.1633,
      "step": 1070
    },
    {
      "epoch": 0.8940397350993378,
      "grad_norm": 3.7865660190582275,
      "learning_rate": 2.1192052980132454e-06,
      "loss": 1.1789,
      "step": 1080
    },
    {
      "epoch": 0.902317880794702,
      "grad_norm": 4.083374500274658,
      "learning_rate": 1.9536423841059603e-06,
      "loss": 1.3788,
      "step": 1090
    },
    {
      "epoch": 0.9105960264900662,
      "grad_norm": 4.276350975036621,
      "learning_rate": 1.7880794701986755e-06,
      "loss": 1.1646,
      "step": 1100
    },
    {
      "epoch": 0.9188741721854304,
      "grad_norm": 3.9180121421813965,
      "learning_rate": 1.6225165562913908e-06,
      "loss": 1.2921,
      "step": 1110
    },
    {
      "epoch": 0.9271523178807947,
      "grad_norm": 3.3273730278015137,
      "learning_rate": 1.456953642384106e-06,
      "loss": 1.2552,
      "step": 1120
    },
    {
      "epoch": 0.9354304635761589,
      "grad_norm": 3.5219268798828125,
      "learning_rate": 1.2913907284768212e-06,
      "loss": 1.1081,
      "step": 1130
    },
    {
      "epoch": 0.9437086092715232,
      "grad_norm": 4.416119575500488,
      "learning_rate": 1.1258278145695367e-06,
      "loss": 1.368,
      "step": 1140
    },
    {
      "epoch": 0.9519867549668874,
      "grad_norm": 3.3764209747314453,
      "learning_rate": 9.602649006622517e-07,
      "loss": 1.1921,
      "step": 1150
    },
    {
      "epoch": 0.9602649006622517,
      "grad_norm": 3.8151872158050537,
      "learning_rate": 7.94701986754967e-07,
      "loss": 1.1407,
      "step": 1160
    },
    {
      "epoch": 0.9685430463576159,
      "grad_norm": 3.3581180572509766,
      "learning_rate": 6.291390728476822e-07,
      "loss": 1.2886,
      "step": 1170
    },
    {
      "epoch": 0.9768211920529801,
      "grad_norm": 4.037650108337402,
      "learning_rate": 4.635761589403974e-07,
      "loss": 1.4057,
      "step": 1180
    },
    {
      "epoch": 0.9850993377483444,
      "grad_norm": 3.937659978866577,
      "learning_rate": 2.980132450331126e-07,
      "loss": 1.3031,
      "step": 1190
    },
    {
      "epoch": 0.9933774834437086,
      "grad_norm": 3.0017154216766357,
      "learning_rate": 1.3245033112582784e-07,
      "loss": 1.0955,
      "step": 1200
    }
  ],
  "logging_steps": 10,
  "max_steps": 1208,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 315646474715136.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
